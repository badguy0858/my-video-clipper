name: AI Pro Animal Factory
on:
  repository_dispatch:
    types: [trigger-compilation]
  workflow_dispatch:

jobs:
  build:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout Code
        uses: actions/checkout@v4

      - name: Install System Engines
        run: |
          sudo apt-get update
          sudo apt-get install -y ffmpeg nodejs fonts-liberation bc
          pip install yt-dlp edge-tts openai-whisper google-api-python-client google-auth-oauthlib google-auth-httplib2

      - name: Setup Subtitle Font
        run: |
          mkdir -p ~/.fonts
          wget -O ~/.fonts/BebasNeue.ttf "https://github.com/google/fonts/raw/main/ofl/bebasneue/BebasNeue-Regular.ttf" || true
          fc-cache -f -v

      - name: Scout & Download Clips (Advanced Bypass)
        env:
          REDDIT_DATA: ${{ secrets.REDDIT_COOKIES }}
          YOUTUBE_DATA: ${{ secrets.YOUTUBE_COOKIES }}
        run: |
          mkdir -p clips
          
          # 1. Prepare Cookie File
          echo "# Netscape HTTP Cookie File" > cookies.txt
          echo "$REDDIT_DATA" >> cookies.txt
          echo "$YOUTUBE_DATA" >> cookies.txt
          sed -i 's/\r//' cookies.txt

          # 2. Modern Mobile User-Agent
          UA="Mozilla/5.0 (iPhone; CPU iPhone OS 17_5 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.5 Mobile/15E148 Safari/604.1"
          
          # 3. Try searching for viral links (Reddit first, then YouTube)
          echo "Finding viral links..."
          
          # Try Reddit JSON (Uses cookies + UA)
          yt-dlp --cookies cookies.txt --user-agent "$UA" \
          --get-url --flat-playlist "https://www.reddit.com/r/AnimalsBeingDerps/top/.json?t=day" | head -n 5 > links.txt || echo "Reddit failed"

          # Fallback: YouTube search via 'mweb' client (Usually bypasses PO Token)
          if [ ! -s links.txt ]; then
            echo "Searching YouTube via mobile client bypass..."
            yt-dlp --user-agent "$UA" --extractor-args "youtube:player_client=mweb,ios" \
            --get-url --flat-playlist "ytsearch10:funny animals shorts" | head -n 5 > links.txt
          fi

          # 4. DOWNLOAD phase
          echo "Downloading MP4 files..."
          # We use --ignore-config and specific clients to avoid the 'sign in' error
          yt-dlp --cookies cookies.txt --user-agent "$UA" \
          --extractor-args "youtube:player_client=mweb,ios" \
          -i -f "bestvideo[ext=mp4][height<=1080]+bestaudio[ext=m4a]/mp4" \
          -a links.txt --no-playlist -o "clips/%(id)s.mp4"

          # Final check: If everything failed, try downloading ONE viral video without cookies
          if [ -z "$(ls -A clips)" ]; then
            echo "Cookies failed, attempting one last anonymous download..."
            yt-dlp "https://www.youtube.com/watch?v=dQw4w9WgXcQ" -f mp4 -o "clips/fallback.mp4"
          fi

      - name: Generate AI Voiceover
        run: |
          edge-tts --text "These animals are having a totally normal day. Wait for the last one, it is hilarious! Subscribe for more daily fun." --write-media vo.mp3

      - name: Standardize and Stitch Clips
        run: |
          mkdir -p processed
          for f in clips/*.mp4; do
            ffmpeg -y -i "$f" -vf "scale=1080:1920:force_original_aspect_ratio=increase,crop=1080:1920,setsar=1,fps=30" \
            -af "aresample=44100,pan=stereo|c0=c0|c1=c1" -c:v libx264 -preset superfast "processed/${f##*/}"
          done
          ls processed/*.mp4 | sed "s/^/file '/;s/$/'/" > list.txt
          ffmpeg -f concat -safe 0 -i list.txt -c copy raw_stitch.mp4
          ffmpeg -i raw_stitch.mp4 -i vo.mp3 -filter_complex "[1:a]adelay=500|500[voice];[0:a][voice]amix=inputs=2:duration=first" -c:v copy stitched_video.mp4

      - name: AI Transcription & Captions
        shell: python
        run: |
          import whisper
          model = whisper.load_model("base")
          result = model.transcribe("stitched_video.mp4", word_timestamps=True)
          with open("captions.ass", "w", encoding="utf-8") as f:
              f.write("[Script Info]\nScriptType: v4.00+\nPlayResX: 1080\nPlayResY: 1920\n\n")
              f.write("[V4+ Styles]\nFormat: Name, Fontname, Fontsize, PrimaryColour, OutlineColour, BorderStyle, Outline, Alignment, MarginV\n")
              f.write("Style: Default,Bebas Neue,100,&H0000FFFF,&H00000000,1,4,2,380\n\n")
              f.write("[Events]\nFormat: Layer, Start, End, Style, Text\n")
              for segment in result['segments']:
                  for word in segment.get('words', []):
                      start, end = word['start'], word['end']
                      text = word['word'].strip().upper()
                      ts = lambda s: f"{int(s//3600)}:{int((s%3600)//60):02d}:{s%60:05.2f}"
                      f.write(f"Dialogue: 0,{ts(start)},{ts(end)},Default,{text}\n")

      - name: Final Render
        run: |
          ffmpeg -y -i stitched_video.mp4 -vf "ass=captions.ass" -c:v libx264 -crf 18 -c:a aac output.mp4

      - name: Post to YouTube
        shell: python
        env:
          ID: ${{ secrets.YT_CLIENT_ID }}
          SEC: ${{ secrets.YT_CLIENT_SECRET }}
          TOK: ${{ secrets.YT_REFRESH_TOKEN }}
        run: |
          import os
          from googleapiclient.discovery import build
          from google.oauth2.credentials import Credentials
          from googleapiclient.http import MediaFileUpload
          creds = Credentials(None, refresh_token=os.environ['TOK'], client_id=os.environ['ID'], client_secret=os.environ['SEC'], token_uri="https://oauth2.googleapis.com/token")
          youtube = build("youtube", "v3", credentials=creds)
          body = {"snippet": {"title": "Funny Animals! ðŸ˜‚ #Shorts", "categoryId": "15"}, "status": {"privacyStatus": "public"}}
          youtube.videos().insert(part="snippet,status", body=body, media_body=MediaFileUpload("output.mp4", chunksize=-1, resumable=True)).execute()

      - name: Upload Artifact
        uses: actions/upload-artifact@v4
        with:
          name: short-video
          path: output.mp4
