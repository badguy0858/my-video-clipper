name: Neil-Style Smart Clipper (Gemini)
on:
  repository_dispatch:
    types: [clip-video]

jobs:
  clip:
    runs-on: ubuntu-latest
    timeout-minutes: 60

    steps:
      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.10'

      - name: Setup FFmpeg
        uses: FedericoCarboni/setup-ffmpeg@v3

      - name: Install Dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y libgl1 libglib2.0-0 fonts-liberation
          pip install --upgrade pip
          # Fix for Face Tracking
          pip install "protobuf==3.20.3"
          # Added google-generativeai for Gemini
          pip install gdown faster-whisper mediapipe opencv-python-headless google-generativeai
          
          mkdir -p ~/.fonts
          wget -q -O ~/.fonts/Montserrat-Bold.ttf \
            "https://github.com/JulietaUla/Montserrat/raw/master/fonts/ttf/Montserrat-Bold.ttf" || true
          fc-cache -f || true

      - name: Download Video
        run: |
          FILE_ID="${{ github.event.client_payload.file_id }}"
          echo "Downloading: $FILE_ID"
          gdown "https://drive.google.com/uc?id=${FILE_ID}" -O raw_input.mp4 || \
          gdown --fuzzy "https://drive.google.com/file/d/${FILE_ID}/view" -O raw_input.mp4
          
          # Standardize video to prevent encoding errors
          ffmpeg -y -i raw_input.mp4 -c:v libx264 -preset fast -crf 24 -c:a aac -b:a 128k -ar 44100 input.mp4
          ls -lh input.mp4

      - name: Run Gemini Smart Clipper
        shell: python
        env:
          GEMINI_API_KEY: ${{ secrets.GEMINI_API_KEY }}
          YOUTUBE_CREDENTIALS: ${{ secrets.YOUTUBE_CREDENTIALS }}
          CHANNEL_TYPE: ${{ github.event.client_payload.channel_type }}
        run: |
          import os
          import json
          import subprocess
          import cv2
          import numpy as np
          import mediapipe as mp
          from faster_whisper import WhisperModel
          import google.generativeai as genai
          import glob
          
          # --- CONFIG ---
          INPUT_VIDEO = "input.mp4"
          OUTPUT_DIR = "clips"
          MAX_CLIPS = 1 # Only 1 clip as requested
          
          # Style Config
          CHANNEL = os.environ.get("CHANNEL_TYPE", "science")
          STYLES = {
              "science": {"color": "&H00FFFF00", "font": 80}, # Yellow
              "podcast": {"color": "&H0000FFFF", "font": 75}, # Gold
              "motivation": {"color": "&H000080FF", "font": 78} # Orange
          }
          style = STYLES.get(CHANNEL, STYLES["science"])
          
          os.makedirs(OUTPUT_DIR, exist_ok=True)
          
          # --- STEP 1: WHISPER TRANSCRIPTION ---
          print("üîä Step 1: Transcribing...")
          model = WhisperModel("medium", compute_type="int8")
          segments, info = model.transcribe(INPUT_VIDEO, word_timestamps=True)
          
          transcript_data = []
          full_text_with_timestamps = ""
          
          for seg in segments:
              # Format: [00:12] Text...
              timestamp = int(seg.start)
              time_str = f"[{timestamp}s]"
              full_text_with_timestamps += f"{time_str} {seg.text}\n"
              
              for word in seg.words:
                  transcript_data.append({
                      "text": word.word.strip(),
                      "start": word.start,
                      "end": word.end
                  })
          
          # Limit context for Gemini (First 20 mins approx to prevent token errors)
          truncated_text = full_text_with_timestamps[:30000]
          
          # --- STEP 2: GEMINI HOOK FINDER ---
          print("üß† Step 2: Gemini AI Analysis...")
          genai.configure(api_key=os.environ["GEMINI_API_KEY"])
          gemini = genai.GenerativeModel('gemini-pro')
          
          prompt = f"""
          You are a viral video editor for TikTok/Shorts.
          Analyze this transcript. Find the ONE absolute most viral, engaging hook segment (30 to 55 seconds long).
          It must stand alone (have a clear start and end topic).
          
          TRANSCRIPT:
          {truncated_text}
          
          Return ONLY valid JSON:
          {{
            "start_time": <integer_seconds>,
            "end_time": <integer_seconds>,
            "reason": "<why_is_this_viral>"
          }}
          """
          
          try:
              response = gemini.generate_content(prompt)
              # Clean response text to ensure JSON
              clean_json = response.text.replace('```json', '').replace('```', '').strip()
              clip_data = json.loads(clean_json)
              print(f"üéØ AI Selected: {clip_data}")
          except Exception as e:
              print(f"‚ùå Gemini Error: {e}. Using fallback.")
              clip_data = {"start_time": 0, "end_time": 58, "reason": "Fallback"}

          start_t = float(clip_data.get("start_time", 0))
          end_t = float(clip_data.get("end_time", 58))
          
          # Ensure duration is valid
          if end_t - start_t < 15: end_t = start_t + 30
          if end_t - start_t > 59: end_t = start_t + 59
          
          # --- STEP 3: EXTRACT & FACE TRACK ---
          print("üé• Step 3: Processing Video...")
          
          clip_name = "final_viral_clip"
          temp_file = f"{OUTPUT_DIR}/{clip_name}_temp.mp4"
          final_file = f"{OUTPUT_DIR}/{clip_name}.mp4"
          
          # Extract Raw Segment
          subprocess.run([
              'ffmpeg', '-y', '-ss', str(start_t), '-t', str(end_t - start_t),
              '-i', INPUT_VIDEO, '-c:v', 'libx264', '-c:a', 'aac', temp_file
          ], check=True)
          
          # Smart Face Tracking (Prioritize Largest Face)
          try:
              cap = cv2.VideoCapture(temp_file)
              width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
              height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
              
              target_w = int(height * 9 / 16)
              if target_w > width: target_w = width
              
              mp_face = mp.solutions.face_detection
              detector = mp_face.FaceDetection(model_selection=1, min_detection_confidence=0.6)
              
              x_positions = []
              
              while True:
                  ret, frame = cap.read()
                  if not ret: break
                  
                  img_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                  results = detector.process(img_rgb)
                  
                  frame_center = width // 2
                  
                  if results.detections:
                      # LOGIC: Find the largest face (Width * Height)
                      largest_face = None
                      max_area = 0
                      
                      for detection in results.detections:
                          bbox = detection.location_data.relative_bounding_box
                          area = bbox.width * bbox.height
                          if area > max_area:
                              max_area = area
                              largest_face = detection
                      
                      if largest_face:
                          bbox = largest_face.location_data.relative_bounding_box
                          # Calculate center of the largest face
                          face_x = (bbox.xmin + (bbox.width / 2)) * width
                          x_positions.append(face_x)
                      else:
                          x_positions.append(frame_center)
                  else:
                      x_positions.append(frame_center)
                      
              cap.release()
              
              # Smooth the movement (Running Average) to prevent jitter
              if x_positions:
                  window_size = 15 # Frames to average
                  smoothed_x = np.convolve(x_positions, np.ones(window_size)/window_size, mode='valid')
                  avg_x = int(np.mean(smoothed_x) if len(smoothed_x) > 0 else width // 2)
              else:
                  avg_x = width // 2
                  
              crop_x = max(0, min(width - target_w, int(avg_x - (target_w / 2))))
              crop_filter = f"crop={target_w}:{height}:{crop_x}:0,scale=1080:1920"
              print(f"‚úÖ Tracking Center X: {avg_x}")
              
          except Exception as e:
              print(f"‚ö†Ô∏è Tracking failed: {e}. Center crop.")
              crop_filter = "crop=ih*(9/16):ih,scale=1080:1920"

          # --- STEP 4: CAPTIONS ---
          print("‚úçÔ∏è Step 4: Burning Captions...")
          ass_path = f"{OUTPUT_DIR}/subs.ass"
          
          # Create ASS Header
          ass_header = f"""[Script Info]
          ScriptType: v4.00+
          PlayResX: 1080
          PlayResY: 1920
          [V4+ Styles]
          Format: Name, Fontname, Fontsize, PrimaryColour, OutlineColour, BackColour, Bold, BorderStyle, Outline, Shadow, Alignment, MarginV
          Style: Default,Montserrat Bold,{style['font']},{style['color'].replace('&H', '&H00')},&H00000000,&H80000000,-1,1,3,0,2,350
          [Events]
          Format: Layer, Start, End, Style, Text
          """
          
          subs_lines = []
          for w in transcript_data:
              if w['start'] >= start_t and w['end'] <= end_t:
                  s_rel = w['start'] - start_t
                  e_rel = w['end'] - start_t
                  
                  def ts(s):
                      h, r = divmod(s, 3600)
                      m, sec = divmod(r, 60)
                      return f"{int(h)}:{int(m):02}:{sec:05.2f}"
                  
                  word_text = w['text'].upper()
                  # Active word color hack
                  line = f"Dialogue: 0,{ts(s_rel)},{ts(e_rel)},Default,{{\\c{style['color']}}}{word_text}"
                  subs_lines.append(line)
                  
          with open(ass_path, "w") as f:
              f.write(ass_header + "\n".join(subs_lines))
              
          # Render
          subprocess.run([
              'ffmpeg', '-y', '-i', temp_file, '-vf', f"{crop_filter},subtitles={ass_path}",
              '-c:v', 'libx264', '-preset', 'fast', '-crf', '23', '-c:a', 'aac', final_file
          ], check=True)
          
          print(f"üéâ Created: {final_file}")

      - name: Upload Artifact
        uses: actions/upload-artifact@v4
        with:
          name: GEMINI-CLIP
          path: clips/*.mp4

      - name: Upload to YouTube
        if: ${{ github.event.client_payload.auto_upload == 'true' }}
        continue-on-error: true
        shell: python
        env:
          YOUTUBE_CREDENTIALS: ${{ secrets.YOUTUBE_CREDENTIALS }}
        run: |
          import os, json, glob
          from google.oauth2.credentials import Credentials
          from googleapiclient.discovery import build
          from googleapiclient.http import MediaFileUpload
          
          creds_json = os.environ.get("YOUTUBE_CREDENTIALS")
          if not creds_json: exit(0)
          
          creds = json.loads(creds_json)
          credentials = Credentials(token=None, refresh_token=creds['refresh_token'],
              token_uri="https://oauth2.googleapis.com/token",
              client_id=creds['client_id'], client_secret=creds['client_secret'])
          
          youtube = build('youtube', 'v3', credentials=credentials)
          clip = glob.glob("clips/*.mp4")[0]
          
          body = {
              "snippet": {"title": "Viral Hook üß† #shorts", "categoryId": "22"},
              "status": {"privacyStatus": "public", "selfDeclaredMadeForKids": False}
          }
          youtube.videos().insert(part="snippet,status", body=body,
              media_body=MediaFileUpload(clip, resumable=True)).execute()
          print("‚úÖ Uploaded to YouTube")
